---
title: ''
output: html_document
---

# Import data

```{r, message=F, warning=F}
library(reshape2)
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(tm)
setwd("~/Repos/reference_games/analysis")

msgs = read.csv("../data/shapeReference/message/shapeReferenceMessage.csv") %>%
  rename(msgTime = time, 
         role = sender)

clks = read.csv("../data/shapeReference/clickedObj/clksWithPhylo.csv") 

subjInfo = read.csv("../data/shapeReference/turk/shapeReference-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid)

mean(subjInfo$totalLength / 1000 / 60)
```

# Pre-processing

There's a bunch of pre-processing we'd like to do. For instance, it would be nice to strip all the meta-commentary (e.g. "oops" after missing, complaining about task, etc.), exclude confused people & non-native english speakers.

```{r}
# For later bar plots
dodge <- position_dodge(width=0.9)

combined <- clks %>% 
  left_join(msgs, by = c("gameid", "roundNum")) %>%
  left_join(subjInfo, by = c("gameid", "role")) %>%
  filter(nativeEnglish != "no" | is.na(nativeEnglish)) %>% 
  filter(confused != "confused") %>%
  group_by(gameid) %>%
  filter(length(unique(roundNum)) == 50) %>%
  ungroup() %>%
  mutate(numOutcome = ifelse(outcome == "true", 1, 0)) %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanChars = nchar(as.character(cleanMsg))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numCleanWords < mean(numCleanWords) + 3*sd(numCleanWords))

paste("num participants:", length(unique(combined$gameid)))
```

# Analysis: How often to listeners talk back?

We're mostly interested in the words that speakers choose to describe colors, but listeners say things, too. For instance, they may ask clarification questions on particularly difficult questions, evoking longer answers. Can we safely exclude listeners from our later analyses for simplicity? Are listeners more likely to peep up on harder trials?

```{r}
ggplot(combined %>% 
         group_by(gameid, role) %>% 
         tally() %>% 
         ungroup() %>% 
         complete(gameid,role, fill = list(n = 0)) %>% # fill 0s for listeners
         group_by(role) %>% 
         summarize(numMessagesSent = mean(n), se = sd(n)/sqrt(length(n))), 
       aes(x = role, y = numMessagesSent)) +
  geom_bar(stat = "identity", position = dodge) +
  geom_errorbar(aes(ymax = numMessagesSent + se, ymin = numMessagesSent - se), 
                    position=dodge, width=0.25) +
  ggtitle("How often do listeners talk back?")
```

So listeners talk more on harder trials, but always *much* less than speakers (an average of 4 messages per game combined to 50 messages by speakers)

We might expect that they'd ask questions more than speakers... Let's count question marks... 

```{r}
ggplot(combined %>% 
         group_by(gameid, role) %>%
         summarise(n = str_count(paste(contents, collapse=" "), fixed("?"))) %>%
         ungroup() %>% 
         complete(gameid,role, fill = list(n = 0)) %>% 
         group_by(role) %>% 
         summarize(numQuestionsAsked = mean(n), se = sd(n)/sqrt(length(n))), 
       aes(x = role, y = numQuestionsAsked)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(ymax = numQuestionsAsked + se, ymin = numQuestionsAsked - se), 
                  position=dodge, width=0.25) +
  ggtitle("how many questions did each player ask?")
```

Yep! Speakers actually use questions, too, to mark uncertainty (e.g. "purple?"), and it actually looks like the number of questions falls off with the difficulty of the trial. Note that a lot of these are meta-questions like "are you there?" or "Are you a robot?" Might want to scrub these later.

# Analysis: Most common words?

```{r}
speakerWords <- combined %>%
  filter(role == "speaker") %>%
  summarize(wordList = paste(cleanMsg, collapse = " "))
sort(table(strsplit(speakerWords$wordList, ' ')), decreasing=T)[1:50]
```

# Are players sticky?

```{r}

```

# Analysis: How good at this are they?

```{r}
(combined %>%
  group_by(gameid, roundNum) %>%
  filter(row_number() == 1) %>%
  group_by(gameid) %>%
  summarize(acc = mean(numOutcome)) %>%
  ungroup() %>%
  summarize(overallAcc = mean(acc), se = sd(acc)/sqrt(length(acc))))
```

# Analysis: How does behavior change over time?

```{r}
ggplot(combined %>% 
    group_by(roundNum, gameid) %>% 
    filter(row_number()==1) %>%
    filter(role == "speaker") %>%
  summarize(meanNumWords = mean(numRawWords)),
  aes(x = roundNum, y = meanNumWords)) +
  geom_line() +
  facet_wrap(~ gameid)
```

Doesn't look like very many people have a trend over time

```{r}
colorConvMod = lmer(numCleanChars ~ roundNum + (1 | gameid), 
     data = combined %>% 
  group_by(gameid, roundNum) %>%
  filter(row_number()==1) %>%
  filter(role == "speaker"))

summary(colorConvMod)
pdf("shapeReferenceFigs/shapeConvMeans.pdf")
ggplot(combined %>% 
  group_by(gameid, roundNum) %>%
  filter(row_number()==1) %>%
  filter(role == "speaker") %>%
  group_by(roundNum) %>% 
  summarize(meanNumChars = mean(numCleanChars), se = sd(numCleanChars)/sqrt(length(numCleanChars))),
  aes(x = roundNum, y = meanNumChars)) +
  geom_point() +
  geom_errorbar(aes(ymax = meanNumChars + se, ymin = meanNumChars - se)) +
  geom_smooth() +
  ggtitle("Do people conventionalize?") +
  theme_bw()
dev.off()

pdf("shapeReferenceFigs/shapeConvRaw.pdf")
ggplot(combined %>% 
  group_by(gameid, roundNum) %>%
  filter(row_number()==1) %>%
  filter(role == "speaker"),
  aes(x = roundNum, y = numCleanChars)) +
  geom_point(alpha = 0.1, position = position_jitter(w = 0.5, h = 0.5)) +
  ylab("# characters used") +
  geom_smooth(method = "lm") +
  ggtitle("Do people conventionalize?") +
  theme_bw()
dev.off()
```

# Entropy

```{r}
library(entropy)

makeDTMatrix <- function(cleanMsg) {
  print(cleanMsg)
  corpus <- Corpus(VectorSource(paste(cleanMsg, collapse = " ")))
  print(as.matrix(DocumentTermMatrix(corpus)))
  return (as.matrix(DocumentTermMatrix(corpus)))
}

withinPair <- combined %>% 
  filter(gameid == "3732-3") %>%
  group_by(gameid) %>%
  summarize(entropy = entropy(colSums(makeDTMatrix(cleanMsg)))) %>%
  mutate(type = "within") %>%
  select(entropy, type, gameid)

acrossPair <- combined %>% 
  group_by(roundNum) %>% 
  summarize(entropy = entropy(colSums(makeDTMatrix(cleanMsg)))) %>%
  mutate(type = "across") %>%
  select(entropy, type)

#pdf("shapeReferenceFigs/entropies.pdf")
ggplot(rbind(withinPair, acrossPair), aes(x = entropy, fill = type)) +
  geom_bar()
#dev.off()
```

# Use shape clusters for something

```{r}
head(combined)

speakerWords <- combined %>%
  group_by(clustL20) %>% 
  summarize(wordList = paste(cleanMsg, collapse = " "))

docs <- lexicalize(speakerWords$wordList)

K <- 10 # num topics

result <- lda.collapsed.gibbs.sampler(docs$documents, K, docs$vocab, 5000, 
                                      0.1, 0.1, compute.log.likelihood=TRUE)

N <- 5 # num docs to display
top.words <- top.topic.words(result$topics, 5, by.score=TRUE)
top.words
topic.proportions <- t(result$document_sums) / colSums(result$document_sums)
topic.proportions <- topic.proportions[sample(1:dim(topic.proportions)[1], N),]
topic.proportions[is.na(topic.proportions)] <-  1 / K
colnames(topic.proportions) <- apply(top.words, 2, paste, collapse=" ")
topic.proportions.df <- melt(cbind(data.frame(topic.proportions),
                                   document=factor(1:N)),
                                   variable.name="topic",
                                   id.vars = "document")
(ggplot(topic.proportions.df, aes(x = topic, y = value, fill=document))
  + ylab("proportion")
  + geom_bar(stat="identity", position = "dodge") 
  + theme(axis.text.x = element_text(angle=90, hjust=1)) 
  + facet_wrap(~ document, ncol=5))

```
