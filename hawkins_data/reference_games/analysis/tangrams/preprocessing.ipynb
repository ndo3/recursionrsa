{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import lots of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import re;\n",
    "import pandas as pd\n",
    "import pylab as pyl\n",
    "import nltk as nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline\n",
    "#enable longer display\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate messages with tangram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get one round from one game to test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_msgs = (pd.read_csv('../../data/tangrams/message/tangramsMessages.csv', escapechar='\\\\')\n",
    "          .query('sender == \"director\"')\n",
    "          .drop('sender', 1))\n",
    "d_msgs['tangramRef'] = \"None\"\n",
    "d_drops = (pd.read_csv('../../data/tangrams/dropObj/tangramsDropObj.csv', escapechar='\\\\'))\n",
    "d_boards = (pd.read_csv('reformattedBoards.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious strategy is to (on a first pass) assume that the tangram the matcher moves in response to a message is the one the message is referring to. The second pass is to skip the ones where we know they got it wrong. We'll probably end up hand-tagging those or using some other strategy depending on how many there are.\n",
    "\n",
    "There are a few obvious problems here:\n",
    "\n",
    "1. The director will sometimes send several messages before the matcher moves anything. So we can't just use the closest move in time... \n",
    "2. instead, we could use the *first* move action after the message and then rule it out so that we won't use it again even if it's the first after later message as well\n",
    "3. **that**, though, also has a problem. Multiple messages are sent per tangram, and some messages are meta-chatter (e.g. \"hello\", \"thanks\", \"good job\", \"this HIT is terrible\"). If we assign the drop actions to the first $N$ messages, we'll have a bunch of actual messages about tangrams that aren't tagged and a bunch of messages **not** about tangrams incorrectly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. this code matches the first N messages on each trial with the first N drop actions... \n",
    "# It really doesn't work at all\n",
    "# d_drops['notUsed'] = True\n",
    "# d_msgs['tangramRef'] = None\n",
    "# for index, row in d_msgs.iterrows():\n",
    "#     msgTime = row['time']    \n",
    "#     postMsgDrops = d_drops.query('notUsed and dropTime > {0}'.format(msgTime))\n",
    "#     firstPostMsgDrop = postMsgDrops[:1]\n",
    "#     d_msgs[index-1:index]['tangramRef'] = firstPostMsgDrop['name']\n",
    "#     firstPostMsgDrop['notUsed'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... we'll do a simpler thing. Check for numbers occuring in the text and look them up in the board data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('[\\W_]+')\n",
    "for index, row in d_msgs.iterrows():\n",
    "    stripedStr = pattern.sub(' ', row.contents)\n",
    "    numbers = [int(s) for s in stripedStr.split() if s.isdigit()]\n",
    "    gameid = row.gameid\n",
    "    roundNum = row.roundNum\n",
    "    if len(numbers) == 1 and 0 < numbers[0] <= 12 :\n",
    "        boardRow = d_boards.query('gameid == \"{0}\" and roundNum == {1} and trueLoc == {2}'\n",
    "                                  .format(gameid, roundNum, numbers[0]))\n",
    "        d_msgs.set_value(index, 'tangramRef', boardRow.tangramName.tolist()[0])\n",
    "d_msgs.to_csv(\"taggedTangrams.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how many we tagged..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 - Counter(d_msgs['tangramRef'])['None'] / float(d_msgs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60% isn't bad for a conservative heuristic! Now we're going to use the tagged data to train a classifier that will make predictions for the other 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "d_nicki = (pd.read_csv('../../data/tangrams/old/oldTangrams.csv')\n",
    "    .query('tangram != \"*\"')\n",
    "    .drop('sender', 1)\n",
    "    .rename(columns = {'tangram' : 'tangramRef'}))\n",
    "d_combined = (d_msgs\n",
    "  .query('tangramRef != \"None\"')\n",
    "  .append(pd.DataFrame(data = d_nicki), ignore_index=True))\n",
    "train_msg, test_msg = train_test_split(d_combined, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline that tokenizes and trains a naive bayes classifier..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largely drawn from [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'vect__stop_words': (None, 'english'),\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3, 1e-4, 1e-5)\n",
    "}\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),#)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='log', penalty='l2',n_iter=5)),\n",
    "                    ])\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf\n",
    "_ = gs_clf.fit(train_msg.contents, train_msg.tangramRef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = predicted == test_msg.tangramRef\n",
    "print(\"test-split accuracy is...\")\n",
    "print(sum(correct)/float(len(correct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = gs_clf.predict(test_msg.contents)\n",
    "test_msg.loc[:, 'predicted'] = predicted\n",
    "test_msg.loc[:, 'correct'] = test_msg['predicted'] == test_msg['tangramRef']\n",
    "test_msg.loc[:, 'maxProb'] = [max(row) for row in gs_clf.predict_proba(test_msg['contents'])]\n",
    "# print(gs_clf.predict_proba(test_msg['contents'])[0])\n",
    "# print(test_msg.ix[:,'maxDecisionFunc'])\n",
    "\n",
    "actualNumPos= float(sum(test_msg['correct']))\n",
    "actualNumNeg= len(test_msg['correct']) - float(sum(test_msg['correct']))\n",
    "\n",
    "TPRs, FPRs = [], []\n",
    "for threshold in np.arange(0, 1, .05) :\n",
    "    # Get the ones that our policy tags as \"correct\"\n",
    "    predYes = test_msg.query('maxProb > {0}'.format(threshold))['correct']\n",
    "    # TPR: number *correct* positive results relative to overall number positive samples \n",
    "    TPRs.append(sum(predYes)/actualNumPos)\n",
    "    # TPR: number *incorrect* positive results relative to overall number negative samples \n",
    "    FPRs.append((len(predYes)-sum(predYes))/actualNumNeg)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect = 'equal')\n",
    "#forceAspect(ax)    \n",
    "ax.plot([0,1], [0,1])\n",
    "ax.plot(FPRs, TPRs)\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are best params?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure()\n",
    "cm = metrics.confusion_matrix(test_msg.tangramRef, predicted)\n",
    "tangramLabels = sorted(list(set(test_msg.tangramRef)))\n",
    "plot_confusion_matrix(cm, tangramLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annotated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../../data/tangrams/old/oldTangrams.csv')\n",
    "\n",
    "# Drop time column\n",
    "d = (d_raw\n",
    "    .copy()\n",
    "    .drop('time', 1)\n",
    "    .query('tangram != \"0\"')\n",
    "    .query('tangram != \"*\"'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['tokens'] = [[word for word in nltk.word_tokenize(sentence.lower()) if word.isalpha()]\n",
    "               for sentence in d['contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['numWords'] = [pd.value_counts(words).sum() for words in d['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 1: Generate file for POS analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['pos'] = [[pos for (key, pos) in nltk.pos_tag(rowTokens, tagset = 'universal')] \n",
    "            for rowTokens in d['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all unique POS labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posSet = set({})\n",
    "for row in d['pos'] :\n",
    "    for pos in row :\n",
    "        posSet.add(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get counts for each POS label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pos in posSet :\n",
    "    colName = pos + \"num\"\n",
    "    d[colName] = [posList.count(pos) for posList in d['pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv for plotting in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(d.drop([\"pos\", \"contents\", \"tokens\"], 1)\n",
    " .to_csv(\"posTagged.csv\", index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 2: Calculate indicator words for tangrams/rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get list of words in first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down to first round\n",
    "d_round1 = d[d['roundNum'] == 1]\n",
    "\n",
    "# Pull out all tokens and collapse into count dict\n",
    "tokenDict = Counter([item for sublist in d_round1['tokens'].tolist()\n",
    "                     for item in sublist])\n",
    "\n",
    "# Pull out all words that occur more than once\n",
    "wordList = [word for (word,count) in tokenDict.items() if count > 1]\n",
    "print(wordList[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all game ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gameidList = pd.unique(d.gameid.ravel()).tolist()\n",
    "print(gameidList[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all tangram names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tangramList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "print(tangramList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to select words & counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getWordCounts(df, gameid, roundNum, tangram = None) :\n",
    "    roundCond = 'roundNum == ' + roundNum\n",
    "    gameidCond = 'gameid == \"' + gameid + '\"'\n",
    "    if(tangram is not None) :\n",
    "        tangramCond = 'tangram == \"' + tangram + '\"'\n",
    "        cond = \" and \".join((roundCond, gameidCond, tangramCond))\n",
    "    else :\n",
    "        cond = \" and \".join((roundCond, gameidCond))\n",
    "    relevantRow = df.query(cond)\n",
    "    return Counter([item for sublist in relevantRow['tokens'].tolist() \n",
    "                    for item in sublist])\n",
    "\n",
    "#creates mini dataframe that grabs the words used in round n for a given tangram and gameid\n",
    "def selectTangramRoundWords(df, tangram, roundNum, gameid):\n",
    "    wordCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "    return wordCounts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to compute PMIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that merging is really costly -- if we need to speed it up, this might be the first target. Can also vectorize the log operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns a table with the all words above 0 PMI and their counts for a given tangram\n",
    "#calculate the probability for words given tangram A ------ p(x|y)\n",
    "def makeMyPMI(df, tangram, roundNum, gameid, totals):\n",
    "\n",
    "    # count words w/in tangram\n",
    "    tangramCounts = getWordCounts(df, gameid, roundNum, tangram)\n",
    "\n",
    "    #total number of words \n",
    "    tangramNumWords = (1 if sum(tangramCounts.values()) == 0 \n",
    "                       else sum(tangramCounts.values()))\n",
    "\n",
    "    #dataframe to compare \n",
    "    indicatorDF = pd.merge(pd.DataFrame(tangramCounts.items(), columns=['word', 'count']),\n",
    "                           pd.DataFrame(totals[\"counts\"].items(), columns=['word', 'totalCount']),\n",
    "                           on='word', how = 'inner')\n",
    "\n",
    "    #calculate PMI without log first. Having trouble with float issues. \n",
    "    indicatorDF['roughPMI'] = ((indicatorDF['count']/tangramNumWords)\n",
    "                                / (indicatorDF['totalCount']/totals[\"numWords\"]))\n",
    "    indicatorDF['logPMI'] = [math.log10(num) for num in indicatorDF['roughPMI']]\n",
    "    \n",
    "    #remove column rough PMI\n",
    "    indicatorDF = indicatorDF.drop('roughPMI', 1)\n",
    "    \n",
    "    return indicatorDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out PMIs & matching rates for all words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do a sloppy optimization by only computing total counts once and only when necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memoize(d, gameid, counts) : \n",
    "    if \"counts\" not in counts : \n",
    "        counts[\"counts\"] = getWordCounts(d, gameid, \"1\")\n",
    "        counts[\"numWords\"] = float(sum(counts[\"counts\"].values()))\n",
    "        return counts\n",
    "    else \n",
    "        return counts\n",
    "\n",
    "with open('matchAndPMI.csv', 'ab') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['word', 'match', 'pmi', 'total'])\n",
    "    for word in wordList :\n",
    "        print(word)\n",
    "        pmi = 0\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for gameid in gameidList:  \n",
    "            memoizedCounts = {}\n",
    "            for tangram in tangramList:\n",
    "                memoizedCounts = memoize(d, gameid, memoizedCounts)\n",
    "                round1WordList = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "                total = total + 1 if word in round1WordList else total\n",
    "                if word in round1WordList :\n",
    "                    PMI_df = makeMyPMI(d, tangram, \"1\", gameid, memoizedCounts)\n",
    "                    pmi = pmi + PMI_df[PMI_df['word'] == word]['logPMI'].tolist()[0]\n",
    "                    round6WordList = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                    match = (match + 1 if (word in round1WordList and word in round6WordList)\n",
    "                             else match)\n",
    "        writer.writerow([word, float(match) / float(total), pmi/total, total])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrap analysis (might want to move to R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab words with highestPMI for a given tangram/gameid\n",
    "def highestPMIWords(d, tangram, roundNum, gameid):\n",
    "    allTangramCounts = {}\n",
    "    allTangramCounts['counts'] = getWordCounts(d, gameid, \"1\")\n",
    "    allTangramCounts['numWords'] = float(sum(allTangramCounts[\"counts\"].values()))\n",
    "\n",
    "    PMIdf = makeMyPMI(d, tangram, roundNum, gameid, allTangramCounts)\n",
    "    #if PMIdf has words, pull out max values, it is empty return it as is\n",
    "    if len(PMIdf.index) > 0:\n",
    "        PMI_values = PMIdf.logPMI.unique()\n",
    "        maxPMI = PMI_values.max()\n",
    "        PMIdf = PMIdf.loc[PMIdf['logPMI'] == maxPMI]\n",
    "        PMIdfword = PMIdf['word']\n",
    "        return PMIdfword.tolist()\n",
    "    else: \n",
    "        return PMIdf\n",
    "\n",
    "numSamples = 1000\n",
    "with open('PMIbootstrap.csv', 'wb') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['sampleNum', 'tangram', 'gameid', 'numCandidates', 'match', 'highest'])\n",
    "    for gameid in gameidList :\n",
    "        for tangram in tangramList :\n",
    "            round1Words = selectTangramRoundWords(d, tangram, \"1\", gameid)\n",
    "            if len(round1Words) > 0:\n",
    "                # First, write highest PMI match\n",
    "                highPMIWords = highestPMIWords(d, tangram, \"1\", gameid)\n",
    "                round6Words = selectTangramRoundWords(d, tangram, \"6\", gameid)\n",
    "                match = np.mean([1 if word in round6Words else 0 for word in highPMIWords ])\n",
    "                writer.writerow([0, tangram, gameid, len(highPMIWords), match, \"highest\"])\n",
    "\n",
    "                # Next, take a bunch of null samples\n",
    "                for i in range(numSamples) :\n",
    "                    randomWord = np.random.choice(round1Words)\n",
    "                    match = np.mean([1 if randomWord in round6Words else 0])\n",
    "                    writer.writerow([i + 1, tangram, gameid, 1, match, \"null\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
