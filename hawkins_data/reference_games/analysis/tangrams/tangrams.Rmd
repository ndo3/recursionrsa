---
title: "tangramsReference"
output: html_document
---

# Import data

```{r}
library(ggplot2)
library(lmer)
library(lmerTest)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(tm)
library(NLP)
setwd("~/Repos/reference_games/analysis/tangrams/")
```

We've already done most of the work using nltk in the ipython notebook, so here we just read in the csv files we created there, do a bit of post-processing, and make plots.

# Result 1: Parts of speech 

```{r}
d = read.csv('posTagged.csv', header =T) %>%
  filter(sender == "director") %>%
  group_by(roundNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(NOUNnum)/sum(numWords),
            verbs = sum(VERBnum)/sum(numWords),
            dets= sum(DETnum)/sum(numWords),
            pronouns = sum(PRONnum)/sum(numWords),
            preps = sum(ADPnum)/sum(numWords),
            adjectives = sum(ADJnum)/sum(numWords),
            adverbs = sum(ADVnum)/sum(numWords)) %>%
  mutate(OTHER = (1 - nouns - verbs - dets - pronouns -
                      preps - adjectives - adverbs)) %>%
  gather(POS, prop, nouns:OTHER) %>%
  select(roundNum, POS, prop) 
  
head(d)

ggplot(d, aes(x = roundNum, y = prop, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw()
```

Same thing but make them add up to number of words so we can also see that decrease over time:

```{r}
d = read.csv('posTagged.csv', header =T) %>%
  filter(sender == "director") %>%
  group_by(roundNum) %>%
  summarize(numWords = sum(numWords),
            nouns = sum(NOUNnum),
            verbs = sum(VERBnum),
            dets= sum(DETnum),
            pronouns = sum(PRONnum),
            preps = sum(ADPnum),
            adjectives = sum(ADJnum),
            adverbs = sum(ADVnum)) %>%
  mutate(OTHER = (numWords - nouns - verbs - dets - pronouns -
                      preps - adjectives - adverbs)) %>%
  gather(POS, total, nouns:OTHER) %>%
  select(roundNum, POS, total) 

head(d)
ggplot(d, aes(x = roundNum, y = total, fill = POS)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw()

```

# Result 2: PMI

Scatter plot:

```{r}
distinctiveness_d <- read.csv("matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences >= 7) %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences, color = a_match)) +
  geom_smooth(method = 'lm') +
  theme_bw() +
  scale_colour_manual(values=cbbPalette)+
  guides(color=FALSE)
```

Alternatively, can do a nonparametric analysis: draw a random word from each tangram/gameid pair and look at the percentage that match with round 6... This gives a null distribution. Then we can take the highest PMI word (or words) for each tangram/gameid pair and look at the percentage of *those* that match. We see that it's much higher than expected under the null.

```{r}
nonparametric_d = read.csv("PMIbootstrap.csv", header = TRUE) %>%
  mutate(PMI = factor(highest, levels = c('null', 'highest'), labels = c('random', 'top')))

highestValAvg = nonparametric_d %>% filter(highest == 'highest') %>% summarize(avg = mean(match))

nonparametric_d %>%
  group_by(sampleNum, PMI) %>%
  summarize(avgMatchRate = mean(match)) %>%
  ungroup() %>%
  ggplot(aes(x = avgMatchRate, fill = PMI)) +
  geom_histogram(binwidth = .0075) +
  geom_vline(aes(xintercept = highestValAvg), color = 'red', linetype = "dashed", size = 2) +
  scale_fill_manual(values = c("black","red")) +
  theme_bw() +
  guides(color=FALSE)
```

# Result 3: Entropy

```{r}
setwd("~/Repos/reference_games/analysis")

tangramMsgs = read.csv("../data/tangrams/message/tangramsMessages.csv") %>%
  rename(msgTime = time, 
         role = sender)

tangramSubjInfo = read.csv("../data/tangrams/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  filter(!is.na(nativeEnglish)) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

mean(tangramSubjInfo$totalLength / 1000 / 60)
```

```{r}
tangramCombined <- tangramMsgs %>%
  left_join(tangramSubjInfo, by = c("gameid", "role")) %>%
  filter(nativeEnglish != "no") %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

length(unique(tangramCombined$gameid))
```

```{r}
pdf("tangramsFigs/wordOverTime.pdf")
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words (by director) per figure") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw() 
dev.off()
```

# Make wordclouds for round 6

```{r}
library(wordcloud)   

oldGrams = read.csv("oldTangrams.csv", quote = '"') %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

textPerGram = oldGrams %>%
  group_by(gameid, tangram) %>%
  filter(tangram != 0) %>%
  filter(roundNum == 6) %>%
  summarize(a = paste(cleanMsg, collapse = " ")) %>%
  group_by(tangram) %>%
  summarize(text = paste(a, collapse = " ")) %>%
  rename(docs = tangram) %>%
  mutate(docs = paste("doc ", docs))

corpus = Corpus(VectorSource(textPerGram$text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('english'))

dtm = DocumentTermMatrix(corpus)

numDocs = dim(dtm)[1]
numTerms = dim(dtm)[2]
  
for(i in 1:numDocs) {
  # pdf(paste("wordcloudForTangram", i, ".pdf", sep = ""))
  freq <- sort(colSums(as.matrix(dtm["1",])), decreasing=TRUE)
  print(entropy(freq))
#   wordcloud(names(freq), freq, min.freq = 1, colors=brewer.pal(6, "Dark2"))   
#   dev.off()
}
```

# Compute across-pair entropy and within-pair entropy

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  group_by(gameid, tangram) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  group_by(tangram, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangram") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangram)) +
  geom_line()
```

Or we could look at both on each half? 

```{r}
library(entropy)

withinPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(gameid, tangram, half) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangram, half) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

withinPair

acrossPair <- oldGrams %>% 
  filter(tangram != 0) %>%
  mutate(half = factor(ifelse(roundNum <= 3, "beg", "end"))) %>%
  group_by(tangram, half) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = c("tangram", "half")) %>%
  gather(type, entropy, acrossEnt, withinEnt)

acrossPair

ggplot(acrossPair,
       aes(x = half, y = entropy, 
           color = tangram, linetype = type, group = interaction(tangram, type))) +
  geom_line()
```

# Copied from nicki's ipython notebook... Need to recreate this myself

```{r}
nullDistributionNums = c(0.2013888888888889,
 0.19444444444444445,
 0.1423611111111111,
 0.1597222222222222,
 0.13541666666666666,
 0.1527777777777778,
 0.13194444444444445,
 0.17708333333333334,
 0.1388888888888889,
 0.1597222222222222,
 0.1597222222222222,
 0.16666666666666666,
 0.1423611111111111,
 0.21180555555555555,
 0.1527777777777778,
 0.16319444444444445,
 0.1840277777777778,
 0.1423611111111111,
 0.18055555555555555,
 0.15625,
 0.17708333333333334,
 0.1701388888888889,
 0.19444444444444445,
 0.1701388888888889,
 0.16666666666666666,
 0.1701388888888889,
 0.1597222222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1388888888888889,
 0.2048611111111111,
 0.16666666666666666,
 0.14583333333333334,
 0.1736111111111111,
 0.16666666666666666,
 0.16666666666666666,
 0.16666666666666666,
 0.15625,
 0.1388888888888889,
 0.1423611111111111,
 0.1527777777777778,
 0.1597222222222222,
 0.2048611111111111,
 0.15625,
 0.15625,
 0.1597222222222222,
 0.1597222222222222,
 0.13541666666666666,
 0.1527777777777778,
 0.16319444444444445,
 0.16666666666666666,
 0.1423611111111111,
 0.16666666666666666,
 0.1736111111111111,
 0.1388888888888889,
 0.18055555555555555,
 0.14930555555555555,
 0.15625,
 0.1701388888888889,
 0.14583333333333334,
 0.15625,
 0.18055555555555555,
 0.1597222222222222,
 0.1597222222222222,
 0.17708333333333334,
 0.13541666666666666,
 0.14583333333333334,
 0.1284722222222222,
 0.1527777777777778,
 0.2048611111111111, 
 0.1527777777777778,
 0.1527777777777778,
 0.1527777777777778,
 0.1423611111111111,
 0.1597222222222222,
 0.1388888888888889,
 0.1701388888888889,
 0.1597222222222222,
 0.1597222222222222,
 0.17708333333333334,
 0.16319444444444445,
 0.14930555555555555,
 0.1597222222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1423611111111111,
 0.15625,
 0.1736111111111111,
 0.1597222222222222,
 0.1701388888888889,
 0.1527777777777778,
 0.1423611111111111,
 0.15625,
 0.1388888888888889,
 0.19791666666666666,
 0.14583333333333334,
 0.17708333333333334,
 0.1388888888888889,
 0.1909722222222222,
 0.14583333333333334,
 0.1875,
 0.16666666666666666,
 0.19444444444444445,
 0.16319444444444445,
 0.15625,
 0.1527777777777778,
 0.1701388888888889,
 0.12152777777777778,
 0.1701388888888889,
 0.13194444444444445,
 0.1597222222222222,
 0.1388888888888889,
 0.1736111111111111,
 0.1527777777777778,
 0.14930555555555555,
 0.16319444444444445,
 0.1527777777777778,
 0.14583333333333334,
 0.13541666666666666,
 0.1875,
 0.1597222222222222,
 0.16666666666666666,
 0.1909722222222222,
 0.1597222222222222,
 0.16319444444444445,
 0.14930555555555555,
 0.1909722222222222,
 0.1527777777777778,
 0.14930555555555555,
 0.1527777777777778,
 0.1736111111111111,
 0.18055555555555555,
 0.1423611111111111,
 0.15625,
 0.1840277777777778,
 0.1388888888888889,
 0.16319444444444445,
 0.14930555555555555,
 0.16319444444444445,
 0.16319444444444445,
 0.1840277777777778,
 0.19791666666666666,
 0.1597222222222222,
 0.1388888888888889,
 0.1736111111111111,
 0.1423611111111111,
 0.13541666666666666,
 0.15625,
 0.1875,
 0.1597222222222222,
 0.16319444444444445,
 0.1597222222222222,
 0.1423611111111111,
 0.1597222222222222,
 0.1875,
 0.13194444444444445,
 0.16319444444444445,
 0.1423611111111111,
 0.16666666666666666,
 0.1388888888888889,
 0.14930555555555555,
 0.1527777777777778,
 0.16319444444444445,
 0.1423611111111111,
 0.1701388888888889,
 0.1388888888888889,
 0.15625,
 0.17708333333333334,
 0.19444444444444445,
 0.1736111111111111,
 0.1701388888888889,
 0.14583333333333334,
 0.16319444444444445,
 0.17708333333333334,
 0.14583333333333334,
 0.1875,
 0.16666666666666666,
 0.17708333333333334,
 0.16666666666666666,
 0.1527777777777778,
 0.1736111111111111,
 0.16666666666666666,
 0.11805555555555555,
 0.18055555555555555,
 0.16666666666666666,
 0.1701388888888889,
 0.1388888888888889,
 0.16666666666666666,
 0.14930555555555555,
 0.1909722222222222,
 0.17708333333333334,
 0.125,
 0.15625,
 0.1701388888888889,
 0.14930555555555555,
 0.11805555555555555,
 0.1597222222222222,
 0.14583333333333334,
 0.16666666666666666,
 0.16319444444444445)

pdf("tangramsFigs/nullDistribution.pdf")
qplot(nullDistributionNums,binwidth = .01) +
  geom_vline(x = 0.2112, color = 'red', size = 4) +
  xlab("probability of match b/w Rounds 1 & 6") +
  ggtitle("Are high PMI words more likely to \n conventionalize than expected by chance?") +
  theme_bw()
dev.off()
```